{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# National Institute of Technology Karnataka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "1b54c774-49b1-4b3f-8e45-1569771bd41a",
    "_uuid": "659a95cf-f1b5-4d65-ba8c-13a9c9a21956",
    "id": "iKJVI6zijIXw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ::  True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Import torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import non-deeplearning libraries\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from glob import glob\n",
    "\n",
    "# Import libraries for images manipulation and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Check if a GPU is available\n",
    "CUDA = torch.cuda.is_available()\n",
    "print(\"CUDA :: \", CUDA)\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW5hF8kzTST6"
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base paper for the VGG net can be found [here](https://arxiv.org/pdf/1409.1556.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "19035019-5e84-49a3-a4dd-743d114db480",
    "_uuid": "5eb7f6d5-679b-4ada-8935-fb759346de20",
    "id": "G7BcVZR1q4U8"
   },
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, hidden = 64, no_of_classes=2, kernel_size=(3,3), padding=1, stride=(1,1)):\n",
    "        super(VGG16, self).__init__()\n",
    "        '''\n",
    "        Initializes the layers for the neral network model. The VGG-16 with Batch Normalization is used\n",
    "        \n",
    "        Arguments:\n",
    "        hidden  --  The number of channels for the first layer. The no of channels for the consequent layers are a multiple of this number\n",
    "        no_of_classes --  The number of output classes\n",
    "        kernel_size  --  Filter Size for the Conv layers\n",
    "        padding  --  Padding for the Conv layers. For SAME conv, padding = (kernel_size - 1)/2\n",
    "        stride  --  Stride for the Conv layers\n",
    "        \n",
    "        Return:\n",
    "        None\n",
    "        '''\n",
    "        # Input Shape is 64x64\n",
    "        # Block 1. It consists of the Conv layers till the first max pooling layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = hidden)\n",
    "        self.conv2 = nn.Conv2d(in_channels=hidden, out_channels=hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features = hidden)\n",
    "        self.max1  = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "\n",
    "        # Block 2. It consists of the Conv layers till the second max pooling layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=hidden, out_channels=2*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features = 2*hidden)\n",
    "        self.conv4 = nn.Conv2d(in_channels=2*hidden, out_channels=2*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features = 2*hidden)\n",
    "        self.max2  = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "\n",
    "        # Block 3. It consists of the Conv layers till the third max pooling layer\n",
    "        self.conv5 = nn.Conv2d(in_channels=2*hidden, out_channels=4*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features = 4*hidden)\n",
    "        self.conv6 = nn.Conv2d(in_channels=4*hidden, out_channels=4*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn6 = nn.BatchNorm2d(num_features = 4*hidden)\n",
    "        self.conv7 = nn.Conv2d(in_channels=4*hidden, out_channels=4*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn7 = nn.BatchNorm2d(num_features = 4*hidden)\n",
    "        self.max3  = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "\n",
    "        # Block 4. It consists of the Conv layers till the fourth max pooling layer\n",
    "        self.conv8  = nn.Conv2d(in_channels=4*hidden, out_channels=8*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn8 = nn.BatchNorm2d(num_features = 8*hidden)\n",
    "        self.conv9  = nn.Conv2d(in_channels=8*hidden, out_channels=8*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn9 = nn.BatchNorm2d(num_features = 8*hidden)\n",
    "        self.conv10 = nn.Conv2d(in_channels=8*hidden, out_channels=8*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn10 = nn.BatchNorm2d(num_features = 8*hidden)\n",
    "        self.max4  = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "\n",
    "        # Block 5. It consists of the Conv layers till the fifth max pooling layer\n",
    "        self.conv11 = nn.Conv2d(in_channels=8*hidden, out_channels=8*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn11 = nn.BatchNorm2d(num_features = 8*hidden)\n",
    "        self.conv12 = nn.Conv2d(in_channels=8*hidden, out_channels=8*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn12 = nn.BatchNorm2d(num_features = 8*hidden)\n",
    "        self.conv13 = nn.Conv2d(in_channels=8*hidden, out_channels=8*hidden, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn13 = nn.BatchNorm2d(num_features = 8*hidden)\n",
    "        self.max5  = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "\n",
    "        # Fully connected layers and Classification\n",
    "        # in_features = dim x dim x 512\n",
    "        # dim = input_shape / 32. Dimensions after the 32x downsampling\n",
    "        self.fc1 = nn.Linear(in_features=2*2*512, out_features = 4096)\n",
    "        self.fc2 = nn.Linear(in_features=4096, out_features = 4096)\n",
    "        self.final_layer = nn.Linear(in_features=4096, out_features=no_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward propogation of the model. The ReLU activations are used for the Conv layers\n",
    "        \n",
    "        Arguments:\n",
    "        x  --  Input to the VGG network\n",
    "        \n",
    "        Returns\n",
    "        y  --  Output of the VGG network\n",
    "        '''\n",
    "        # Block 1\n",
    "        x_1 = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        x_1 = nn.ReLU()(self.bn2(self.conv2(x_1)))\n",
    "        x_1 = self.max1(x_1)\n",
    "\n",
    "        # Block 2\n",
    "        x_2 = nn.ReLU()(self.bn3(self.conv3(x_1)))\n",
    "        x_2 = nn.ReLU()(self.bn4(self.conv4(x_2)))\n",
    "        x_2 = self.max2(x_2)\n",
    "\n",
    "        # Block 3\n",
    "        x_3 = nn.ReLU()(self.bn5(self.conv5(x_2)))\n",
    "        x_3 = nn.ReLU()(self.bn6(self.conv6(x_3)))\n",
    "        x_3 = nn.ReLU()(self.bn7(self.conv7(x_3)))\n",
    "        x_3 = self.max3(x_3)\n",
    "\n",
    "        # Block 4\n",
    "        x_4 = nn.ReLU()(self.bn8(self.conv8(x_3)))\n",
    "        x_4 = nn.ReLU()(self.bn9(self.conv9(x_4)))\n",
    "        x_4 = nn.ReLU()(self.bn10(self.conv10(x_4)))\n",
    "        x_4 = self.max4(x_4)\n",
    "\n",
    "        # Block 5\n",
    "        x_5 = nn.ReLU()(self.bn11(self.conv11(x_4)))\n",
    "        x_5 = nn.ReLU()(self.bn12(self.conv12(x_5)))\n",
    "        x_5 = nn.ReLU()(self.bn13(self.conv12(x_5)))\n",
    "        x_5 = self.max5(x_5)\n",
    "\n",
    "        # The 32x downsampled image is flattened and passed through Linear and Classification layers\n",
    "        x_flat = nn.Flatten()(x_5)\n",
    "        x_fc = self.fc1(x_flat)\n",
    "        x_fc = self.fc2(x_fc)\n",
    "        y = self.final_layer(x_fc)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "e3d3485f-8fee-43e6-a752-c6e20a1baa62",
    "_uuid": "f2d8216c-7a90-4a9f-a264-4ce05b98d020",
    "id": "1fieINJkp_N0"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters for the model\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate Model and mount it on the relavent device\n",
    "vgg16model = VGG16()\n",
    "vgg16model.to(device)\n",
    "\n",
    "# Relavent PATHs for saving the model and loading the data\n",
    "DATASET_FOLDER_PATH = '../input/breast-histopathology-images/'\n",
    "CHECKPOINT_PATH = ''\n",
    "LOAD_PICKLE = True\n",
    "PICKLE_PATH = '../input/inputfiles/images_files_data.pkl'\n",
    "\n",
    "# Instantiate Optimzer and Loss Function. The L2 regularization penalty factor is 0.0005\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg16model.parameters(), lr=learning_rate, weight_decay=5*(10**(-4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataframe\n",
    "\n",
    "Load the dataframe from the pkl file containing the filenames and the label<br/>\n",
    "If the pkl file does not exist, assign **LOAD_PICKLE** as False in the previous snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "c7e7d151-d35d-4d3a-a532-7ea7c460d087",
    "_uuid": "be378f6f-2321-4230-9fef-93f3c3d520ad",
    "id": "TRQ0r0Rcot94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Complete. Total number of files =  555048\n"
     ]
    }
   ],
   "source": [
    "if LOAD_PICKLE == False:\n",
    "    imagePatches = glob(DATASET_FOLDER_PATH + '**/*.png', recursive=True)\n",
    "    print('Total number of files = ', len(imagePatches))\n",
    "    y = []\n",
    "    for img in imagePatches:\n",
    "        if img.endswith('class0.png'):\n",
    "            y.append(0)\n",
    "        elif img.endswith('class1.png'):\n",
    "            y.append(1)\n",
    "    images_df = pd.DataFrame()\n",
    "    images_df[\"images\"] = imagePatches\n",
    "    images_df[\"labels\"] = y\n",
    "    images_df.to_pickle(PICKLE_PATH)\n",
    "else:\n",
    "    images_df = pd.read_pickle(PICKLE_PATH)\n",
    "    print('Read Complete. Total number of files = ', len(images_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "c9dfc41a-7c86-4483-b931-9ccff8751bf6",
    "_uuid": "4fae5f01-c3ac-42e7-81e2-660fdd02b5ee",
    "id": "6ZRkbt6WpZ_0",
    "outputId": "be8e1894-8bb4-4774-b6b6-e9f6226e99ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/breast-histopathology-images/14304/1/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/breast-histopathology-images/14304/1/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/breast-histopathology-images/14304/1/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/breast-histopathology-images/14304/1/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/breast-histopathology-images/14304/1/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images  labels\n",
       "0  ../input/breast-histopathology-images/14304/1/...       1\n",
       "1  ../input/breast-histopathology-images/14304/1/...       1\n",
       "2  ../input/breast-histopathology-images/14304/1/...       1\n",
       "3  ../input/breast-histopathology-images/14304/1/...       1\n",
       "4  ../input/breast-histopathology-images/14304/1/...       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images_df = images_df[:40000]\n",
    "images_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "8757ac25-02fb-414b-9855-2ef97a40d0b7",
    "_uuid": "1fcf293d-1771-4930-ad29-8032f9ce907c",
    "id": "7c1dpD9Ipe0Y",
    "outputId": "58de3f98-2f6f-4a37-eff0-776744992f0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0    397476\n",
       "1    157572\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df.groupby(\"labels\")[\"labels\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "52a27d81-b108-4795-9bb6-4b2119a8bd18",
    "_uuid": "84dd8c94-026f-453c-9a73-527a4e2d138c",
    "id": "uJUvk7W6pfZR",
    "outputId": "da2ec51e-3591-4636-ef74-3de398c05a06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444038, 111010)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting data into train and val\n",
    "train, val = train_test_split(images_df, stratify=images_df.labels, test_size=0.2)\n",
    "len(train), len(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "9dd599a1-6332-48f3-8151-2db026c63de4",
    "_uuid": "8288e422-0350-45e2-a904-b8cb55025096",
    "id": "FF6dcMohpl0C"
   },
   "outputs": [],
   "source": [
    "class IDC_Dataset(Dataset):\n",
    "    def __init__(self, data_from,transform=None):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Initialize the Dataloader\n",
    "        \n",
    "        Arguments:\n",
    "        data_from  --  The dataframe with the filenames and the labels\n",
    "        transform  --  The transformation to be applied to the input for normalization and/or data augmentation\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "        self.data_from = data_from.values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Calculate the number of file/data-points in the dataframe\n",
    "        \n",
    "        Arguments:\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        Number of files\n",
    "        '''\n",
    "        return len(self.data_from)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Based on the input index, reads a row(filename and the label) and outputs the loaded image and the label as a tensor\n",
    "        \n",
    "        Arguments:\n",
    "        index  --  The index of the dataframe row to be loaded\n",
    "        \n",
    "        Returns:\n",
    "        image  --  The image tensor of dimension [batch,channels,height,width]\n",
    "        label  --  The label for the image loaded in a tensor format. Either 0 or 1\n",
    "        '''\n",
    "        img_path,label = self.data_from[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "c8d2d81d-71fd-4de1-82a5-2ad2f05c68cd",
    "_uuid": "66f1d780-206e-41c1-9c7e-8d869bd77eec",
    "id": "4txcAOp3p0Fn"
   },
   "outputs": [],
   "source": [
    "# Transformations for the traning and the validation DataLoader\n",
    "training_transformation = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Resize((64,64)), \n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "validation_transformation = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Resize((64,64)),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "# Instantiate the Data Loader classes\n",
    "dataset_train = IDC_Dataset(train, transform=training_transformation)\n",
    "dataset_valid = IDC_Dataset(val,transform=validation_transformation)\n",
    "\n",
    "# Instantiate the iterators\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_valid = DataLoader(dataset_valid, batch_size=batch_size//2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "cf166005-3555-4529-96de-94b901325407",
    "_uuid": "5e2dbe4d-b2ea-4148-9dab-2b7d703bed14",
    "id": "odNO41v1r0FY",
    "outputId": "36c49cd7-9e88-4a38-e711-21b393f1425c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Completed\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 50\n",
    "resume_epoch = 50\n",
    "# loss_train = []\n",
    "if resume_epoch == 0 :\n",
    "    print(\"Start Traning\")\n",
    "elif resume_epoch == num_epochs:\n",
    "    print(\"Traning Completed\")\n",
    "else:\n",
    "    print(\"Resume Training. Last Epoch \", resume_epoch)\n",
    "    checkpoint = torch.load('../input/secondckpt/' + str(resume_epoch) + '.tar', map_location=torch.device(device))\n",
    "    vgg16model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "train_length = len(loader_train)\n",
    "val_length   = len(loader_valid)\n",
    "for epoch in range(resume_epoch, num_epochs):\n",
    "    # intialize the metrics and losses to 0\n",
    "    loss_epoch = 0\n",
    "    val_loss = 0\n",
    "    accuracy_train = 0\n",
    "    accuracy_val = 0\n",
    "    f1_score_train = 0\n",
    "    f1_score_val = 0\n",
    "    jaccard_train = 0\n",
    "    jaccard_val = 0\n",
    "    # Enable traning mode \n",
    "    vgg16model.train()\n",
    "    for (images, labels) in loader_train:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = vgg16model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch += loss.item()\n",
    "        \n",
    "        # Conversion to numpy\n",
    "        output_numpy = np.argmax(outputs.clone().detach().cpu().numpy(), axis=1).ravel()\n",
    "        labels_numpy = labels.cpu().numpy().ravel()\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy_train += accuracy_score(labels_numpy, output_numpy)\n",
    "        f1_score_train += f1_score(labels_numpy, output_numpy, average='weighted')\n",
    "        jaccard_train += jaccard_score(labels_numpy, output_numpy, average='weighted')\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Print the outputs\n",
    "    print(\"Epoch : \", epoch + 1, \" Loss : \", loss_epoch/train_length, \" Accuracy : \", accuracy_train/train_length,\" F1/Dice : \", f1_score_train/train_length, \" Jaccard/IoU : \", jaccard_train/train_length)\n",
    "    if((epoch+1) % 10 == 0):\n",
    "        print('Saving model stats')\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': vgg16model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_epoch}, CHECKPOINT_PATH + str(epoch+1) + '.tar')\n",
    "    loss_train.append(loss_epoch)\n",
    "\n",
    "    # Validation\n",
    "    # Enable evaluation mode\n",
    "    vgg16model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader_valid:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = vgg16model(images)\n",
    "            \n",
    "            # Conversion to numpy\n",
    "            output_numpy = np.argmax(outputs.clone().detach().cpu().numpy(), axis=1).ravel()\n",
    "            labels_numpy = labels.cpu().numpy().ravel()\n",
    "            \n",
    "            # Metrics\n",
    "            accuracy_val += accuracy_score(labels_numpy, output_numpy)\n",
    "            f1_score_val += f1_score(labels_numpy, output_numpy, average='weighted')\n",
    "            jaccard_val += jaccard_score(labels_numpy, output_numpy, average='weighted')\n",
    "\n",
    "            val_loss += criterion(outputs, labels)\n",
    "        save_file = open('save.txt', 'a')\n",
    "        print(\"Validation Loss : \", val_loss.item()/val_length, \" Accuracy : \", accuracy_val/val_length, \" F1/Dice : \", f1_score_val/val_length, \" Jaccard/IoU : \", jaccard_val/val_length, file=save_file)\n",
    "        print(\" \", file=save_file)\n",
    "        save_file.close\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "poKVm5Q6TSVA",
    "outputId": "26718737-f091-4e23-fd2d-78f1132f7188"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7a3e48c636430daf202da08d235d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3470.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy :  0.895064841498559  Jaccard/IoU :  0.8213702044697411\n",
      "F1/Dice :  0.8973128812161003  Precision :  0.9077056461101652  Recall :  0.895064841498559\n"
     ]
    }
   ],
   "source": [
    "accuracy_test  = 0\n",
    "f1_score_test   = 0\n",
    "jaccard_test  = 0\n",
    "precision_test = 0\n",
    "recall_test = 0\n",
    "pred_list  = []\n",
    "label_list = []\n",
    "\n",
    "# Load the model state dictionary\n",
    "checkpoint = torch.load('../input/finalckpt/' + str(50) + '.tar', map_location=torch.device(device))\n",
    "vgg16model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Enable testing mode\n",
    "vgg16model.eval()\n",
    "test_length = len(loader_valid)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(loader_valid):\n",
    "        # Seat the images and labels to the designated device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Calculate model output\n",
    "        outputs = vgg16model(images)\n",
    "        \n",
    "        # Convert output to numpy variables\n",
    "        output_numpy = np.argmax(outputs.clone().detach().cpu().numpy(), axis=1).ravel()\n",
    "        labels_numpy = labels.cpu().numpy().ravel()\n",
    "        \n",
    "        # Metric Calculation\n",
    "        accuracy_test += accuracy_score(labels_numpy, output_numpy)\n",
    "        f1_score_test  += f1_score(labels_numpy, output_numpy, average='weighted')\n",
    "        jaccard_test += jaccard_score(labels_numpy, output_numpy, average='weighted')\n",
    "        precision_test += precision_score(labels_numpy, output_numpy, average='weighted')\n",
    "        recall_test += recall_score(labels_numpy, output_numpy, average='weighted')\n",
    "        \n",
    "        pred_list.append(output_numpy)\n",
    "        label_list.append(labels_numpy)\n",
    "    print(\" Accuracy : \", accuracy_test/test_length, \" Jaccard/IoU : \", jaccard_test/test_length)\n",
    "    print(\"F1/Dice : \", f1_score_test/test_length, \" Precision : \", precision_test/test_length, \" Recall : \", recall_test/test_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kRH04L2QTSVH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[71751,  7744],\n",
       "       [ 3908, 27607]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = np.concatenate(label_list)\n",
    "pred_list = np.concatenate(pred_list)\n",
    "cm = confusion_matrix(label_list.ravel(), pred_list.ravel())\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2xi7QhXETSVM",
    "outputId": "0441499f-649c-4cbf-dedc-1a3a5aeb64d4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm, interpolation='nearest')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels(['']+['0', '1'])\n",
    "ax.set_yticklabels(['']+['0', '1'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
